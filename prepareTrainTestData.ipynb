{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "cross_val = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all file names from the los folder. This folder contains all the rescaled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES = np.array([])\n",
    "ALL_FILES = glob.glob(\"../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los/*.dat\")\n",
    "len(ALL_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all active region names. Check the number of active regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE = np.array([], dtype=np.int8)\n",
    "REST = np.array([])\n",
    "\n",
    "for F in ALL_FILES:\n",
    "    \n",
    "    \"\"\"extract the file name from the path name\"\"\"\n",
    "    FILE_NAME = os.path.split(F)[-1]\n",
    "    \n",
    "    \"\"\"position of the first _\"\"\"\n",
    "    start = FILE_NAME.index('_')\n",
    "    rest = FILE_NAME[start + 1:]\n",
    "    REST = np.append(REST, rest)\n",
    "    \n",
    "    \n",
    "    ACTIVE_REGION = FILE_NAME[0:start]\n",
    "    ACTIVE = np.append(ACTIVE, ACTIVE_REGION)\n",
    "\n",
    "\"\"\"Select the unique active region names\"\"\"    \n",
    "UNIQUE_ACTIVE = np.unique(ACTIVE)\n",
    "print(len(UNIQUE_ACTIVE)) # number of active regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within an active region, select only those file names which have six hours time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PATHS = np.array([]) # all file paths with six hours time difference\n",
    "\n",
    "\"\"\"\n",
    "Iterate over each active region\n",
    "\"\"\"\n",
    "\n",
    "for i in range(len(UNIQUE_ACTIVE)):\n",
    "    \n",
    "    \"\"\"All file paths for a given active region\"\"\"\n",
    "    PATHS = glob.glob(\"../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los/%s_*.dat\" %UNIQUE_ACTIVE[i])\n",
    "    \n",
    "    FILE_NAME = os.path.split(PATHS[0])[-1]\n",
    "    \n",
    "    \"\"\"first _ position\"\"\"\n",
    "    start = FILE_NAME.index('_')\n",
    "    \n",
    "    \"\"\"last _ position\"\"\"\n",
    "    end = FILE_NAME.index('_', start+1)\n",
    "    \n",
    "    TIMESTAMP = FILE_NAME[start+1:end]\n",
    "    \n",
    "    \"\"\"Timestamp of the first file in the i-th active region\"\"\"\n",
    "    TIME0 = datetime.datetime.strptime(TIMESTAMP,'%Y%m%dT%H%M')\n",
    "    dTS0 = time.mktime(TIME0.timetuple())\n",
    "    ALL_PATHS = np.append(ALL_PATHS, PATHS[0])\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterate over the rest of the files in the i-th active region \n",
    "    \"\"\"\n",
    "    \n",
    "    for j in range(1, len(PATHS)):\n",
    "        FILE_NAME = os.path.split(PATHS[j])[-1]\n",
    "        start = FILE_NAME.index('_')\n",
    "        end = FILE_NAME.index('_', start+1)\n",
    "        TIMESTAMP = FILE_NAME[start+1:end]\n",
    "        TIME = datetime.datetime.strptime(TIMESTAMP,'%Y%m%dT%H%M')\n",
    "        dTS = time.mktime(TIME.timetuple())\n",
    "        \n",
    "        \"\"\"Time difference (in minutes) between the first file and this file\"\"\"\n",
    "        DIFF = int(dTS0 - dTS)/60\n",
    "        if (DIFF % 60 == 0):\n",
    "            ALL_PATHS = np.append(ALL_PATHS, PATHS[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of active regions for the reduced file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVE = np.array([], dtype=np.int8)\n",
    "REST = np.array([])\n",
    "\n",
    "for F in ALL_PATHS:\n",
    "    FILE_NAME = os.path.split(F)[-1]\n",
    "    start = FILE_NAME.index('_')\n",
    "    rest = FILE_NAME[start + 1:]\n",
    "    REST = np.append(REST, rest)\n",
    " \n",
    "    ACTIVE_REGION = FILE_NAME[0:start]\n",
    "    ACTIVE = np.append(ACTIVE, ACTIVE_REGION)\n",
    "\n",
    "UNIQUE_ACTIVE = np.unique(ACTIVE)\n",
    "print(len(UNIQUE_ACTIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the train and test regions. Active region with max(year) > 2013 belongs in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REGIONS = np.array([])\n",
    "TEST_REGIONS = np.array([])\n",
    "\n",
    "for i in range(len(UNIQUE_ACTIVE)):\n",
    "    PATHS = ALL_PATHS\n",
    "    #PATHS = glob.glob(\"../Shared/Magnetogram_Regression/Data/Processed/Valid_Magnetograms/los2/%s_*.png\" %UNIQUE_ACTIVE[i])\n",
    "    YEARS = np.zeros(len(PATHS), dtype=int)\n",
    "    \n",
    "    for j in range(0, len(PATHS)):\n",
    "        FILE_NAME = os.path.split(PATHS[j])[-1]\n",
    "        start = FILE_NAME.index('_')\n",
    "        year = FILE_NAME[start+1:start+1+4]\n",
    "        YEARS[j] = year\n",
    "    if (max(YEARS) <= 2018):\n",
    "        TRAIN_REGIONS = np.append(TRAIN_REGIONS, UNIQUE_ACTIVE[i])\n",
    "    elif(max(YEARS) > 2018):\n",
    "        TEST_REGIONS = np.append(TEST_REGIONS, UNIQUE_ACTIVE[i])\n",
    "\n",
    "assert(len(UNIQUE_ACTIVE) == (len(TRAIN_REGIONS) + len(TEST_REGIONS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the train and test file names from the train and test active regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TRAINING_FILES = np.array([])\n",
    "FINAL_TEST_FILES = np.array([])\n",
    "\n",
    "for i in range(0, len(ALL_PATHS)):\n",
    "    FILE_NAME = os.path.split(ALL_PATHS[2])[-1]\n",
    "    start = FILE_NAME.index('_')\n",
    "    ACTIVE_REGION = FILE_NAME[0:start]\n",
    "    if ACTIVE_REGION in TRAIN_REGIONS:\n",
    "        FINAL_TRAINING_FILES = np.append(FINAL_TRAINING_FILES, ALL_PATHS[i])\n",
    "    if ACTIVE_REGION in TEST_REGIONS:\n",
    "        FINAL_TEST_FILES = np.append(FINAL_TEST_FILES, ALL_PATHS[i])\n",
    "        \n",
    "print(len(FINAL_TRAINING_FILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crosscheck the final train and test years from the train and test file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_TRAINING_YEARS = np.array([], dtype=int)\n",
    "\n",
    "for FTR in FINAL_TRAINING_FILES:\n",
    "    FILE_NAME = os.path.split(FTR)[-1]\n",
    "    start = FILE_NAME.index('_')\n",
    "    year = FILE_NAME[start+1:start+1+4]\n",
    "    TEMP_TRAINING_YEARS = np.append(TEMP_TRAINING_YEARS, year)\n",
    "TRAINING_YEARS = np.unique(TEMP_TRAINING_YEARS)\n",
    "\n",
    "TEMP_TEST_YEARS = np.array([], dtype=int)\n",
    "\n",
    "for FTE in FINAL_TEST_FILES:\n",
    "    FILE_NAME = os.path.split(FTE)[-1]\n",
    "    start = FILE_NAME.index('_')\n",
    "    year = FILE_NAME[start+1:start+1+4]\n",
    "    TEMP_TEST_YEARS = np.append(TEMP_TEST_YEARS, year)\n",
    "TEST_YEARS = np.unique(TEMP_TEST_YEARS)\n",
    "\n",
    "print(TRAINING_YEARS)\n",
    "print(TEST_YEARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate flaring and nonflaring train regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FLARING_TRAIN_REGIONS = np.array([])\n",
    "TEMP_NONFLARING_TRAIN_REGIONS = np.array([])\n",
    "\n",
    "FLARING_TRAIN_REGIONS = np.array([])\n",
    "NONFLARING_TRAIN_REGIONS = np.array([])\n",
    "\n",
    "\n",
    "for i in range(len(FINAL_TRAINING_FILES)):\n",
    "    PATH = FINAL_TRAINING_FILES[i]\n",
    "    \n",
    "    NAME = os.path.split(PATH)[-1]\n",
    "    \n",
    "    FLAG = NAME[-5]\n",
    "    \n",
    "    start = NAME.index('_')\n",
    "    ACTIVE_REGION = NAME[0:start]\n",
    "    \n",
    "    if (FLAG == '1'):\n",
    "        TEMP_FLARING_TRAIN_REGIONS = np.append(TEMP_FLARING_TRAIN_REGIONS, ACTIVE_REGION)\n",
    "    else:\n",
    "        TEMP_NONFLARING_TRAIN_REGIONS = np.append(TEMP_NONFLARING_TRAIN_REGIONS, ACTIVE_REGION)\n",
    "        \n",
    "FLARING_TRAIN_REGIONS = np.unique(TEMP_FLARING_TRAIN_REGIONS)\n",
    "NONFLARING_TRAIN_REGIONS = np.unique(TEMP_NONFLARING_TRAIN_REGIONS)\n",
    "\n",
    "if (len(FLARING_TRAIN_REGIONS) % cross_val != 0):\n",
    "    for i in range(cross_val - len(FLARING_TRAIN_REGIONS) % cross_val):\n",
    "        FLARING_TRAIN_REGIONS = np.append(FLARING_TRAIN_REGIONS, FLARING_TRAIN_REGIONS[-1])\n",
    "    \n",
    "if (len(NONFLARING_TRAIN_REGIONS) % cross_val != 0):\n",
    "    for i in range(cross_val - len(NONFLARING_TRAIN_REGIONS) % cross_val):\n",
    "        NONFLARING_TRAIN_REGIONS = np.append(NONFLARING_TRAIN_REGIONS, NONFLARING_TRAIN_REGIONS[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "#### `Use this block if flaring and nonflaring regions are available from the begining =====>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLARING_VALIDATION_REGIONS = ['1028', '1041', '1066', '115', '1256', '1350', '1449', '1461',\n",
    "       '1464', '1582', '1603', '1621', '1722', '1724', '1750', '1834',\n",
    "       '1879', '1946', '1996', '2040', '211', '2137', '2186', '2191',\n",
    "       '2193', '2220', '2227', '2362', '245', '2693', '2739', '2748',\n",
    "       '2760', '2790', '2878', '2887', '2920', '3048', '3056', '3258',\n",
    "       '3263', '3291', '3295', '3321', '3341', '3364', '3366', '3376',\n",
    "       '3437', '345', '3497', '3535', '3563', '3587', '362', '3686',\n",
    "       '3688', '3721', '3740', '3766', '377', '3779', '3784', '3793',\n",
    "       '3804', '3813', '3836', '3856', '3879', '394', '4000', '401',\n",
    "       '407', '4071', '4097', '4138', '415', '4186', '4197', '4231',\n",
    "       '4294', '437', '4639', '4698', '4781', '4874', '49', '4920', '495',\n",
    "       '4955', '5011', '5026', '5107', '514', '5144', '5233', '5298',\n",
    "       '54', '5415', '5446', '5447', '5526', '5637', '5653', '5673',\n",
    "       '5885', '667', '746', '753', '814', '856', '878', '892']\n",
    "\n",
    "NONFLARING_TRAIN_REGIONS = ['1', '1001', '1019', '1021', '1026', '104', '1062', '1075', '1079',\n",
    "       '1080', '1090', '1092', '1093', '1113', '1119', '1120', '1124',\n",
    "       '114', '1149', '1186', '12', '1210', '1221', '1237', '1249',\n",
    "       '1275', '1278', '128', '1300', '1303', '1309', '1312', '1313',\n",
    "       '1318', '1338', '1339', '1342', '1348', '135', '1367', '1390',\n",
    "       '1391', '1396', '1399', '1405', '1422', '1424', '1425', '1447',\n",
    "       '1455', '1457', '1465', '1484', '1488', '1514', '1520', '1528',\n",
    "       '156', '1573', '1578', '1596', '1628', '1632', '1634', '1642',\n",
    "       '1644', '1653', '1662', '1677', '1688', '1705', '1711', '1715',\n",
    "       '1727', '1795', '1819', '1832', '1863', '1866', '187', '1873',\n",
    "       '1886', '1892', '1903', '1931', '1951', '1970', '1979', '1990',\n",
    "       '2007', '2011', '2017', '2021', '2028', '2039', '2044', '2047',\n",
    "       '2059', '2061', '2069', '2098', '2106', '2109', '2112', '2130',\n",
    "       '218', '2181', '220', '224', '2240', '2245', '226', '2262', '2291',\n",
    "       '2306', '2314', '2337', '2338', '2341', '2342', '2344', '2352',\n",
    "       '2353', '2360', '2380', '2387', '241', '2411', '2414', '2433',\n",
    "       '2450', '2460', '2469', '2489', '2492', '2501', '2504', '2511',\n",
    "       '252', '2520', '2541', '2557', '256', '2571', '2573', '2581',\n",
    "       '2583', '2585', '2587', '259', '2597', '26', '2625', '2634',\n",
    "       '2651', '2661', '2672', '2677', '2685', '2691', '2696', '2718',\n",
    "       '2727', '2732', '2733', '2737', '274', '2749', '2750', '2758',\n",
    "       '2779', '279', '2822', '2832', '284', '2852', '2904', '2912',\n",
    "       '2952', '2964', '2966', '2968', '297', '2981', '2984', '3012',\n",
    "       '3019', '3022', '3031', '3032', '3049', '3066', '3068', '3082',\n",
    "       '3097', '3098', '3114', '3115', '3119', '3122', '3129', '3149',\n",
    "       '3154', '3194', '3195', '3199', '3217', '3220', '323', '3240',\n",
    "       '3244', '3247', '3248', '325', '3259', '3273', '3293', '3330',\n",
    "       '3336', '3368', '3371', '3400', '3432', '3448', '3457', '3461',\n",
    "       '3473', '3474', '3483', '3515', '3542', '3560', '3586', '3601',\n",
    "       '3608', '3620', '3631', '3647', '3668', '367', '3711', '3753',\n",
    "       '38', '3823', '3824', '3826', '3843', '3845', '3848', '3901',\n",
    "       '3907', '3912', '3921', '3926', '3957', '3978', '3982', '3985',\n",
    "       '3996', '4025', '4038', '4040', '4042', '4073', '4075', '4076',\n",
    "       '4088', '4108', '4111', '4123', '4131', '4133', '4156', '4166',\n",
    "       '4190', '4201', '4205', '421', '4218', '4252', '4256', '4265',\n",
    "       '4272', '4287', '429', '4296', '4328', '4335', '4351', '4375',\n",
    "       '4379', '438', '4383', '4390', '4398', '4399', '443', '4438',\n",
    "       '4440', '4447', '4448', '4455', '4469', '4478', '4502', '451',\n",
    "       '4530', '4536', '4539', '4543', '4551', '4552', '4556', '4566',\n",
    "       '4574', '4576', '4579', '4580', '4591', '4603', '4616', '4640',\n",
    "       '466', '4667', '4673', '4702', '4704', '4711', '4724', '4734',\n",
    "       '475', '4751', '4760', '4761', '4764', '4767', '4783', '4799',\n",
    "       '480', '4802', '4851', '4862', '4864', '4868', '4872', '4882',\n",
    "       '4888', '4889', '4908', '4921', '4932', '4954', '4962', '4963',\n",
    "       '4973', '4978', '5005', '5012', '5028', '5036', '5039', '504',\n",
    "       '5073', '5075', '5103', '5111', '5112', '5118', '5135', '5140',\n",
    "       '5151', '5152', '5183', '5198', '5212', '5229', '5230', '5246',\n",
    "       '5249', '5265', '5284', '5315', '532', '5337', '5342', '5347',\n",
    "       '5351', '5354', '5366', '5374', '5385', '540', '5456', '5462',\n",
    "       '5467', '5472', '5484', '5490', '5492', '5500', '5518', '5534',\n",
    "       '5535', '5537', '5545', '5549', '5559', '556', '5571', '5586',\n",
    "       '5596', '5598', '5627', '5635', '5644', '5658', '5677', '57',\n",
    "       '5724', '5739', '5758', '5783', '5789', '5807', '5808', '5811',\n",
    "       '5818', '5823', '5848', '5852', '587', '5890', '5894', '5908',\n",
    "       '5919', '5927', '598', '602', '605', '606', '610', '622', '639',\n",
    "       '643', '650', '661', '681', '684', '685', '686', '695', '702',\n",
    "       '71', '713', '714', '759', '764', '765', '798', '803', '805',\n",
    "       '812', '843', '847', '850', '851', '869', '875', '903', '909',\n",
    "       '92', '921', '926', '927', '932', '948', '956', '970', '971',\n",
    "       '973', '975', '976', '982', '997']\n",
    "\n",
    "FLARING_TRAIN_REGIONS = np.array(FLARING_TRAIN_REGIONS)\n",
    "NONFLARING_TRAIN_REGIONS = np.array(NONFLARING_TRAIN_REGIONS)\n",
    "\n",
    "if (len(FLARING_TRAIN_REGIONS) % cross_val != 0):\n",
    "    for i in range(cross_val - len(FLARING_TRAIN_REGIONS) % cross_val):\n",
    "        FLARING_TRAIN_REGIONS = np.append(FLARING_TRAIN_REGIONS, FLARING_TRAIN_REGIONS[-1])\n",
    "    \n",
    "if (len(NONFLARING_TRAIN_REGIONS) % cross_val != 0):\n",
    "    for i in range(cross_val - len(NONFLARING_TRAIN_REGIONS) % cross_val):\n",
    "        NONFLARING_TRAIN_REGIONS = np.append(NONFLARING_TRAIN_REGIONS, NONFLARING_TRAIN_REGIONS[-1])\n",
    "        \n",
    "        \n",
    "ALL_REGIONS = np.concatenate([FLARING_TRAIN_REGIONS, NONFLARING_TRAIN_REGIONS])\n",
    "\n",
    "FINAL_TRAINING_FILES = glob.glob('../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los/*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "validationRegions = ['1209', '1321', '1500', '1638', '1806', '1807', '1907', '1930',\n",
    "       '1993', '1999', '2372', '2491', '2519', '2546', '2635', '2636',\n",
    "       '2673', '2716', '2809', '3311', '3344', '3520', '3580', '3730',\n",
    "       '384', '3877', '3894', '392', '393', '3941', '3999', '4344',\n",
    "       '4396', '4817', '4941', '5127', '5186', '5541', '5692', '5738',\n",
    "       '5745', '637', '750', '8', '833', '899', '902', '940', '1038', '1046', '107', '1089', '1126', '1133', '116', '1168',\n",
    "       '1171', '1183', '1271', '1345', '1353', '1389', '1410', '145',\n",
    "       '146', '1471', '1483', '1492', '1527', '1549', '1557', '1558',\n",
    "       '1574', '1611', '1613', '1658', '1669', '1672', '1697', '1701',\n",
    "       '1744', '175', '1756', '1845', '185', '1877', '1893', '190',\n",
    "       '1949', '1959', '1962', '198', '2026', '2037', '2110', '2117',\n",
    "       '2121', '2123', '2131', '2143', '2166', '2173', '2178', '2203',\n",
    "       '2270', '2358', '2366', '2400', '2420', '2502', '2522', '2533',\n",
    "       '2543', '2560', '2598', '2599', '2605', '2619', '2663', '2711',\n",
    "       '2735', '2825', '2861', '2875', '2922', '2945', '2954', '2955',\n",
    "       '2999', '3028', '3103', '318', '3205', '3246', '3252', '3267',\n",
    "       '327', '3286', '3288', '3309', '3323', '3326', '3415', '3420',\n",
    "       '3481', '3490', '3513', '355', '3604', '3635', '364', '3648',\n",
    "       '3700', '3703', '3719', '3741', '3785', '3821', '3874', '3942',\n",
    "       '3965', '3974', '4011', '403', '4065', '4092', '4093', '414',\n",
    "       '4228', '4284', '4288', '43', '4315', '4321', '4337', '4397',\n",
    "       '4424', '444', '4454', '4466', '4477', '4505', '4523', '4541',\n",
    "       '4549', '4559', '46', '4610', '4623', '4655', '4661', '4678',\n",
    "       '4718', '4726', '4792', '4800', '4900', '4942', '4943', '4969',\n",
    "       '4991', '4995', '5002', '5004', '5022', '5051', '5054', '51',\n",
    "       '5113', '5163', '5208', '5275', '5293', '5355', '5375', '538',\n",
    "       '5387', '5413', '5422', '5521', '5543', '5544', '5550', '5577',\n",
    "       '5618', '5678', '5710', '5718', '5750', '576', '5772', '580',\n",
    "       '5820', '5831', '5856', '5865', '5880', '5916', '618', '625',\n",
    "       '640', '652', '662', '674', '705', '712', '725', '740', '794',\n",
    "       '824', '853', '854', '86', '867', '900', '913', '918', '925',\n",
    "       '950', '986']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the flaring and nonflaring train regions into five equal parts. First four parts will be the TRAIN regions and the last parts will be the VALIDATION regions. Repeat the steps for 5-fold cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL = len(np.concatenate(np.split(FLARING_TRAIN_REGIONS,cross_val)[0:cross_val - 1]))\n",
    "FLV = len(np.split(FLARING_TRAIN_REGIONS, cross_val)[-1])\n",
    "\n",
    "NL = len(np.concatenate(np.split(NONFLARING_TRAIN_REGIONS,cross_val)[0:cross_val - 1]))\n",
    "NLV = len(np.split(NONFLARING_TRAIN_REGIONS, cross_val)[-1])\n",
    "\n",
    "R_FL_TRAIN = np.empty((cross_val, FL), dtype = 'S8')\n",
    "R_FL_VALIDATION = np.empty((cross_val, FLV), dtype = 'S8')\n",
    "\n",
    "R_NFL_TRAIN = np.empty((cross_val, NL), dtype = 'S8')\n",
    "R_NFL_VALIDATION = np.empty((cross_val, NLV), dtype = 'S8')\n",
    "\n",
    "for i in range(cross_val):\n",
    "    FLARING_TRAIN_REGIONS = np.roll(FLARING_TRAIN_REGIONS, len(FLARING_TRAIN_REGIONS)//cross_val)\n",
    "    NONFLARING_TRAIN_REGIONS = np.roll(NONFLARING_TRAIN_REGIONS, len(NONFLARING_TRAIN_REGIONS)//cross_val)\n",
    "    \n",
    "    R_FL = np.split(FLARING_TRAIN_REGIONS, cross_val)\n",
    "    R_NFL = np.split(NONFLARING_TRAIN_REGIONS, cross_val)\n",
    "    \n",
    "    T_FL = R_FL[0:cross_val - 1]\n",
    "    V_FL = R_FL[-1]\n",
    "    \n",
    "    T_NFL = R_NFL[0:cross_val - 1]\n",
    "    V_NFL = R_NFL[-1]\n",
    "    \n",
    "    R_FL_TRAIN[i] = np.concatenate(T_FL)\n",
    "    R_FL_VALIDATION[i] = V_FL\n",
    "    \n",
    "    R_NFL_TRAIN[i] = np.concatenate(T_NFL)\n",
    "    R_NFL_VALIDATION[i] = V_NFL\n",
    "    \n",
    "print(R_FL_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_NF_TRAIN = {}\n",
    "NF_TRAIN = {}\n",
    "F_TRAIN = {}\n",
    "\n",
    "\n",
    "F_VALIDATION = np.array([])\n",
    "NF_VALIDATION = np.array([])\n",
    "\n",
    "CV_F_TRAIN = {}\n",
    "CV_NF_TRAIN = {}\n",
    "CV_F_VALIDATION = {}\n",
    "CV_NF_VALIDATION = {}\n",
    "\n",
    "for j in range(cross_val):\n",
    "    TEMP_F_TRAIN = np.array([])\n",
    "    TEMP_NF_TRAIN = np.array([])\n",
    "    TEMP_F_VALIDATION = np.array([])\n",
    "    TEMP_NF_VALIDATION = np.array([])\n",
    "    for i in range(len(FINAL_TRAINING_FILES)):\n",
    "        NAME = os.path.split(FINAL_TRAINING_FILES[i])[-1]\n",
    "        start = NAME.index('_')\n",
    "        ACTIVE_REGION = NAME[0:start]\n",
    "        if ACTIVE_REGION in R_FL_TRAIN[j]:\n",
    "            TEMP_F_TRAIN = np.append(TEMP_F_TRAIN, FINAL_TRAINING_FILES[i])\n",
    "        elif ACTIVE_REGION in R_NFL_TRAIN[j]:\n",
    "            TEMP_NF_TRAIN = np.append(TEMP_NF_TRAIN, FINAL_TRAINING_FILES[i])\n",
    "        elif ACTIVE_REGION in R_FL_VALIDATION[j]:\n",
    "            TEMP_F_VALIDATION = np.append(TEMP_F_VALIDATION, FINAL_TRAINING_FILES[i])\n",
    "        elif ACTIVE_REGION in R_NFL_VALIDATION[j]:\n",
    "            TEMP_NF_VALIDATION = np.append(TEMP_NF_VALIDATION, FINAL_TRAINING_FILES[i])\n",
    "\n",
    "    CV_F_TRAIN[str(j)] = TEMP_F_TRAIN\n",
    "    CV_NF_TRAIN[str(j)] = TEMP_NF_TRAIN\n",
    "    CV_F_VALIDATION[str(j)] = TEMP_F_VALIDATION\n",
    "    CV_NF_VALIDATION[str(j)] = TEMP_NF_VALIDATION\n",
    "\n",
    "\n",
    "TRAIN = {}\n",
    "VALIDATION = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.shuffle(CV_NF_TRAIN[str(i)])\n",
    "    np.random.shuffle(CV_F_TRAIN[str(i)])\n",
    "    np.random.shuffle(CV_NF_VALIDATION[str(i)])\n",
    "    np.random.shuffle(CV_F_VALIDATION[str(i)])\n",
    "\n",
    "    \n",
    "\"\"\"  \n",
    "for i in range(5):\n",
    "    N = len(CV_NF_TRAIN[str(i)])/len(CV_F_TRAIN[str(i)])\n",
    "    D = len(CV_NF_TRAIN[str(i)]) - len(CV_F_TRAIN[str(i)])*N\n",
    "    TEMP = CV_F_TRAIN[str(i)]\n",
    "    for j in range(N-1):\n",
    "        CV_F_TRAIN[str(i)] = np.append(CV_F_TRAIN[str(i)], TEMP)\n",
    "    CV_F_TRAIN[str(i)] = np.append(CV_F_TRAIN[str(i)], TEMP[0:D])\n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "for i in range(cross_val):\n",
    "    TRAIN[str(i)] = np.append(CV_F_TRAIN[str(i)], CV_NF_TRAIN[str(i)])\n",
    "    np.random.shuffle(TRAIN[str(i)])\n",
    "    VALIDATION[str(i)] = np.append(CV_F_VALIDATION[str(i)], CV_NF_VALIDATION[str(i)])\n",
    "    np.random.shuffle(VALIDATION[str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_all.npy', TRAIN)\n",
    "np.save('validation_all.npy', VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PATHS = np.append(TRAIN['0'], VALIDATION['0'])\n",
    "\n",
    "MEAN = 0.0\n",
    "\n",
    "for i in range(len(ALL_PATHS)):\n",
    "    IMAGE = np.load(ALL_PATHS[i])\n",
    "    MEAN += np.mean(IMAGE)\n",
    "    \n",
    "mean = MEAN / len(ALL_PATHS)\n",
    "\n",
    "sum = 0.0\n",
    "N = 256*256*len(ALL_PATHS)\n",
    "\n",
    "for F in ALL_PATHS:\n",
    "    IMAGE = np.load(F)\n",
    "    sum += ((IMAGE - mean)**2).sum()\n",
    "\n",
    "std = np.sqrt(sum/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temp section\n",
    "\n",
    "allPaths = np.array([], dtype='')\n",
    "for activeRegion in regions:\n",
    "    pathsString = \"../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los2/%s_*.dat\" %activeRegion\n",
    "    paths = glob.glob(pathsString)\n",
    "    np.concatenate(allPaths, paths)\n",
    "\n",
    "MEAN = 0.0\n",
    "\n",
    "for i in range(len(allPaths)):\n",
    "    IMAGE = np.load(allPaths[i])\n",
    "    MEAN += np.mean(IMAGE)\n",
    "    \n",
    "mean = MEAN / len(allPaths)\n",
    "\n",
    "sum = 0.0\n",
    "N = 256*256*len(allPaths)\n",
    "\n",
    "for F in allPaths:\n",
    "    IMAGE = np.load(F)\n",
    "    sum += ((IMAGE - mean)**2).sum()\n",
    "\n",
    "std = np.sqrt(sum/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.dump('mean_all.dat')\n",
    "std.dump('std_all.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES = glob.glob(\"../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los2/*.dat\")\n",
    "for i in range()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
