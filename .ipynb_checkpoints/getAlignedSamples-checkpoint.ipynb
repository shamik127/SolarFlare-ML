{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    VALIDATION = np.load('validation.npy')\n",
    "    mean = np.load('mean.dat')\n",
    "    std = np.load('std.dat')\n",
    "    \n",
    "    return (VALIDATION, mean, std)\n",
    "\n",
    "def load_final_model(N = None):\n",
    "    if (N == None):\n",
    "        num = np.array([], dtype = np.int8)\n",
    "        search_path = 'inception_with_generator/cross_vals_*.npy'\n",
    "        files = glob.glob(search_path)\n",
    "        if files == []:\n",
    "            print('found no files!')\n",
    "        else:\n",
    "            for f in files:\n",
    "                n = int(os.path.split(f)[-1][11:-4])\n",
    "                num = np.append(num, n)\n",
    "            N = np.max(num)\n",
    "            tss_path = 'inception_with_generator/cross_vals_%s.npy' %(N)\n",
    "            tss = np.load(tss_path)\n",
    "            print(tss[0])\n",
    "    model_path = 'inception_with_generator/model_%d.h5' %(N)\n",
    "    model = load_model(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_validation_data_for_one_cross_val(VALIDATION_PATHS, j):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for f in VALIDATION_PATHS.item().get(str(j)):\n",
    "        this_x = np.load(f)\n",
    "        this_x = this_x.reshape(256, 256, 1)\n",
    "        this_x = ((this_x - mean) / std)\n",
    "        x_data.append(this_x)\n",
    "        y_data.append(get_y_val(f))\n",
    "    x_data = np.array(x_data, dtype = np.float32)\n",
    "    y_data = np.array(y_data, dtype = np.float32)\n",
    "    return (x_data, y_data)\n",
    "\n",
    "\n",
    "def get_sorted_data_for_particular_active_region(allSortedPaths, active_region_number, only_x = False):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for f in allSortedPaths.get(str(active_region_number)):\n",
    "        this_x = np.load(f)\n",
    "        this_x = this_x.reshape(256, 256, 1)\n",
    "        this_x = this_x.reshape(this_x.shape[0], this_x.shape[1], 1)\n",
    "        this_x = ((this_x - mean) / std)\n",
    "        x_data.append(this_x)\n",
    "        y_data.append(get_y_val(f))\n",
    "    \n",
    "    x_data = np.array(x_data, dtype = np.float32)\n",
    "    y_data = np.array(y_data, dtype = np.float32)\n",
    "    \n",
    "    return (x_data, y_data)\n",
    "\n",
    "def get_y_val(file):\n",
    "    \"\"\"\n",
    "    Get the y value of an image file\n",
    "    \"\"\"\n",
    "    return float(os.path.split(file)[-1][-5])\n",
    "\n",
    "def get_classification_regions_for_one_crossval(crossValidationFile, i):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the classification regions corresponding to the machine predicted values \n",
    "    Parameters: =====>\n",
    "    1) The cross validation file\n",
    "    2) The cross validation number\n",
    "    \"\"\"\n",
    "    \n",
    "    testData = crossValidationFile.item().get(str(i))\n",
    "    yPredArr = np.array([])\n",
    "    yTestArr = np.array([])\n",
    "    \n",
    "    for i in range(len(testData)):\n",
    "        xData = np.load(testData[i])\n",
    "        xData = (xData - mean)/std\n",
    "        xData = xData.reshape(1, 256, 256, 1)\n",
    "        yTest = float(get_flag(testData[i]))\n",
    "        yPred = model.predict(xData)\n",
    "        \n",
    "        yTestArr = np.append(yTestArr, yTest)\n",
    "        yPredArr = np.append(yPredArr, yPred)\n",
    "        \n",
    "    yPredArrNew = [1.0 if (y > 0.5) else 0.0 for y in yPredArr]\n",
    "    print(yPredArrNew[0:100])\n",
    "    print(yTestArr[0:100])\n",
    "    \n",
    "    \n",
    "    TP = np.array([], dtype=np.int8)\n",
    "    TN = np.array([], dtype=np.int8)\n",
    "    FP = np.array([], dtype=np.int8)\n",
    "    FN = np.array([], dtype=np.int8)\n",
    "\n",
    "    for i in range(len(yPredArrNew)):\n",
    "        imgFile = testData[i]\n",
    "        if (yTestArr[i] == 0.0):\n",
    "            if (yPredArrNew[i] == 0.0):\n",
    "                TN = np.append(TN, imgFile)\n",
    "            else:\n",
    "                FP = np.append(FP, imgFile)\n",
    "        else:\n",
    "            if (yPredArrNew[i] == 0.0):\n",
    "                FN = np.append(FN, imgFile)\n",
    "            else:\n",
    "                TP = np.append(TP, imgFile)\n",
    "    return (TP, TN, FP, FN)\n",
    "        \n",
    "def plot_from_array(filePaths, number_of_plots):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes an array ARR which contains the index values to be plotted. FILE_PATHS contains the array of all the images whose indexes are represented by the array ARR\n",
    "    Parameters: =====>\n",
    "    1) ARR\n",
    "    2) FILE_PATHS\n",
    "    3) number_of_plots\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, math.ceil(number_of_plots/3.0)*5 + math.ceil(number_of_plots/3.0)))\n",
    "    for i in range(number_of_plots):\n",
    "        img = np.load(filePaths[i])\n",
    "        plt.subplot(math.ceil(number_of_plots/3.0),3,i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    \n",
    "\n",
    "def plot_classification_regions(VALIDATION, i, plot_index = 'TP', num_plots = 30):\n",
    "    \"\"\"\n",
    "    Requires four parameters\n",
    "    1) The cross validation file : VALIDATION\n",
    "    2) The cross validation index : i\n",
    "    3) the classification type : TP/TN/FP/FN, default = 'TP'\n",
    "    3) number of plots : default = 30\n",
    "    \n",
    "    Expects global variables : mean and std\n",
    "    \"\"\"\n",
    "    \n",
    "    (TP, TN, FP, FN) = get_classification_regions_for_one_crossval(VALIDATION, i)\n",
    "    V = VALIDATION.item().get(str(i))\n",
    "    if (plot_index == 'TP'):\n",
    "        N = len(TP)\n",
    "        minimum = min(num_plots, N)\n",
    "        plot_from_array(TP, V, minimum)\n",
    "    elif (plot_index == 'TN'):\n",
    "        N = len(TN)\n",
    "        minimum = min(num_plots, N)\n",
    "        plot_from_array(TN, V, minimum)\n",
    "    elif (plot_index == 'FP'):\n",
    "        N = len(FP)\n",
    "        minimum = min(num_plots, N)\n",
    "        plot_from_array(FP, V, minimum)\n",
    "    else:\n",
    "        N = len(FN)\n",
    "        minimum = min(num_plots, N)\n",
    "        plot_from_array(FN, V, minimum)\n",
    "\n",
    "def return_unique_active_regions(ARR):\n",
    "    \"\"\"\n",
    "    Takes one cross validation file and return the unique active regions belonging to the 0-th cross validation region\n",
    "    \"\"\"\n",
    "    \n",
    "    ACTIVE = np.array([], dtype=np.int8)\n",
    "    FILES = ARR.get(str(0))\n",
    "    \n",
    "    for F in FILES:\n",
    "        ACTIVE_REGION = get_active_region(F)\n",
    "        ACTIVE = np.append(ACTIVE, ACTIVE_REGION)\n",
    "    uniqueActive = np.unique(ACTIVE)\n",
    "    \n",
    "    return uniqueActive\n",
    "\n",
    "\n",
    "def get_global_min_max_flux_values(allSortedPaths, uniqueActive):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the sorted paths dictionary and the unique returns the global maximum and minimum magnetic flux values\n",
    "    Parameters: =====>\n",
    "    1) allSortedPaths\n",
    "    2) uniqueActive\n",
    "    \"\"\"\n",
    "    \n",
    "    f_data = np.array([])\n",
    "    \n",
    "    for i in range(len(uniqueActive[0:180])):\n",
    "        \n",
    "        FILES = allSortedPaths.get(str(i))\n",
    "        for k in range(len(FILES)):\n",
    "            x_data = np.load(FILES[k])\n",
    "            f_data = np.append(f_data, np.sum(np.abs(x_data)))        \n",
    "\n",
    "    global_f_max = np.max(f_data)\n",
    "    global_f_min = np.min(f_data)\n",
    "    \n",
    "    return (global_f_max, global_f_min)\n",
    "\n",
    "def sort_active_region_files_wrt_time(CROSS_VALIDATION_FILE):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the cross validation file and returns the sorted paths dictionary. The individual keys of the dictionary corresponds to the different active regions.\n",
    "    \"\"\"\n",
    "    \n",
    "    uniqueActive = return_unique_active_regions(CROSS_VALIDATION_FILE)\n",
    "    #print(len(uniqueActive))\n",
    "    allSortedPaths = {}\n",
    "    \n",
    "    for i in range(len(uniqueActive)):\n",
    "\n",
    "        PATHS = glob.glob(\"../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los/%s_*.dat\" %uniqueActive[i])\n",
    "        TIME_ARR = []\n",
    "        for j in range(len(PATHS)):\n",
    "\n",
    "            FILE_NAME = os.path.split(PATHS[j])[-1]\n",
    "            start = FILE_NAME.index('_')\n",
    "            end = FILE_NAME.index('_', start+1)\n",
    "            TIMESTAMP = FILE_NAME[start+1:end]\n",
    "            TIME = datetime.datetime.strptime(TIMESTAMP,'%Y%m%dT%H%M')\n",
    "            dTS = time.mktime(TIME.timetuple())\n",
    "\n",
    "            TIME_ARR.append(dTS)\n",
    "        TIME_ARR.sort()\n",
    "        #print(TIME_ARR)\n",
    "\n",
    "        SORTED_PATHS = [None]*len(TIME_ARR)\n",
    "\n",
    "        for k in range(len(TIME_ARR)):\n",
    "            FILE_NAME = os.path.split(PATHS[k])[-1]\n",
    "            start = FILE_NAME.index('_')\n",
    "            end = FILE_NAME.index('_', start+1)\n",
    "            TIMESTAMP = FILE_NAME[start+1:end]\n",
    "            TIME = datetime.datetime.strptime(TIMESTAMP,'%Y%m%dT%H%M')\n",
    "            dTS = time.mktime(TIME.timetuple())\n",
    "            idx = TIME_ARR.index(dTS)\n",
    "            SORTED_PATHS[idx] = PATHS[k]\n",
    "\n",
    "        allSortedPaths[uniqueActive[i]] = SORTED_PATHS\n",
    "        \n",
    "    return allSortedPaths\n",
    "\n",
    "def get_global_max_min_pixel_values(allSortedPaths, uniqueActive):\n",
    "    \n",
    "    global_max_arr = np.array([])\n",
    "    global_min_arr = np.array([])\n",
    "\n",
    "    max_data = np.array([])\n",
    "    min_data = np.array([])\n",
    "    \n",
    "    for i in range(len(uniqueActive[0:180])):    \n",
    "        FILES = allSortedPaths.get(str(i))\n",
    "        for k in range(len(FILES)):\n",
    "            x_data = np.load(FILES[k])\n",
    "            max_data = np.append(max_data, np.max(x_data))\n",
    "            min_data = np.append(min_data, np.min(x_data))\n",
    "        \n",
    "    global_f_max = np.max(max_data)\n",
    "    global_f_min = np.min(min_data)\n",
    "    \n",
    "    return (global_f_max, global_f_min)\n",
    "\n",
    "\n",
    "def get_flag(filePath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes a file path and returns the active region flag\n",
    "    \"\"\"\n",
    "    NAME = os.path.split(filePath)[-1]\n",
    "    FLAG = NAME[-5]\n",
    "    return FLAG\n",
    "\n",
    "def image_mag_flux(img):\n",
    "    \"\"\"\n",
    "    Takes one image path and returns the total unsigned magnetic flux of the image\n",
    "    Parameters =====>\n",
    "    1) img : the path of the image\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(img))   \n",
    "    \n",
    "def get_mag_flux_array_for_one_active_region(allSortedPaths, i):\n",
    "    \"\"\"\n",
    "    returns an array of magnetic flux values for the i-th active region belonging to the allSortedPaths dictionary\n",
    "    Parameters =====>\n",
    "    1) \n",
    "    \"\"\"\n",
    "    f_data = np.array([])\n",
    "    ARR = allSortedPaths.get(str(i))\n",
    "    for m in range(len(ARR)):\n",
    "        x_data = np.load(ARR[m])\n",
    "        f_data = np.append(f_data, np.sum(np.abs(x_data)))\n",
    "    return f_data\n",
    "\n",
    "def convert_to_dictionary(DICT):\n",
    "    \"\"\"\n",
    "    Convert dictionary type to proper dictionary.\n",
    "    Parameters =====>\n",
    "    1) DICT : The dictionary type dictionary\n",
    "    \"\"\"\n",
    "    NEW_DICT = {}\n",
    "    for key in DICT.item():\n",
    "        NEW_DICT[key] = DICT.item().get(key)\n",
    "    return NEW_DICT\n",
    "    \n",
    "\n",
    "def get_filename(FILE):\n",
    "    \"\"\"\n",
    "    Takes the path of an image and returns the file name of the image\n",
    "    \"\"\"\n",
    "    return os.path.split(FILE)[-1]\n",
    "\n",
    "def get_active_region(FILE):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes the path of an image and returns the active region of the image:\n",
    "    \n",
    "    Parameters ======>\n",
    "    1) FILE : String\n",
    "    \n",
    "    \"\"\"\n",
    "    FILE_NAME = get_filename(FILE)\n",
    "    start = FILE_NAME.index('_')\n",
    "    ACTIVE_REGION = FILE_NAME[0:start]\n",
    "    \n",
    "    return ACTIVE_REGION\n",
    "\n",
    "def distorted_path_to_original_path(PATH):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes the path of a resized image and return the corresponding path of the original image\n",
    "    \n",
    "    Parameters =====>\n",
    "    1) PATH : String\n",
    "    \n",
    "    \"\"\"\n",
    "    FILE_NAME = get_filename(PATH)\n",
    "    ACTIVE_REGION = get_active_region(PATH)\n",
    "    FLAG = get_flag(PATH)\n",
    "        \n",
    "    if (FLAG == '1'):\n",
    "        TYPE = 'flaring'\n",
    "    else:\n",
    "        TYPE = 'nonflaring'\n",
    "        \n",
    "    FILE_STRING = \"../Shared/Magnetogram_Regression/Data/Processed/Valid_Magnetograms/%s/%s/%s\" %(TYPE, ACTIVE_REGION, FILE_NAME)\n",
    "    \n",
    "    return FILE_STRING\n",
    "    \n",
    "    \n",
    "def distorted_paths_to_original_paths(DISTORTED_PATHS):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes the paths array of resized images and returns the corresponding paths array of the original images\n",
    "    \n",
    "    Parameters =====>\n",
    "    1) DISTORTED_PATHS : Array of Strings\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ORIGINAL_PATHS = np.array([])\n",
    "    for F in DISTORTED_PATHS:\n",
    "        ORIGINAL_PATH = distorted_path_to_original_path(F)\n",
    "        ORIGINAL_PATHS = np.append(ORIGINAL_PATHS, ORIGINAL_PATH)\n",
    "        \n",
    "    return ORIGINAL_PATHS\n",
    "\n",
    "def distorted_paths_dictionary_to_original_paths_dictionary(DISTORTED_PATHS_DICT, uniqueActive):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes the entire path dictionary of the resized images and returns the corresponding path array dictionary of the original images\n",
    "    \n",
    "    Parameters =====>\n",
    "    1) DISTORTED_PATHS_DICT : Dictionary of Array of Strings\n",
    "    2) uniqueActiveRegions : String Array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ORIGINAL_PATHS_DICT = {}\n",
    "    #print(DISTORTED_PATHS_DICT.keys())\n",
    "    \n",
    "    for i in range(len(uniqueActive)):\n",
    "        DISTORTED_PATHS = DISTORTED_PATHS_DICT.get(str(i))\n",
    "        ORIGINAL_PATHS = distorted_paths_to_original_paths(DISTORTED_PATHS)\n",
    "        ORIGINAL_PATHS_DICT[str(i)] = ORIGINAL_PATHS\n",
    "    #print(ORIGINAL_PATHS_DICT.keys())    \n",
    "    return ORIGINAL_PATHS_DICT\n",
    "        \n",
    "\n",
    "def distorted_cross_validation_to_original_cross_validation(CROSS_VALIDATION_FILE):\n",
    "    ORIGINAL_CROSS_VALIDATION_FILE = {}\n",
    "    for key in CROSS_VALIDATION_FILE.keys():\n",
    "        FILES = CROSS_VALIDATION_FILE.get(key)\n",
    "        ORIGINAL_FILES = distorted_paths_to_original_paths(FILES)\n",
    "        ORIGINAL_CROSS_VALIDATION_FILE[key] = ORIGINAL_FILES\n",
    "    \n",
    "    return ORIGINAL_CROSS_VALIDATION_FILE\n",
    "\n",
    "    \n",
    "def active_regions_time_series_plots(VALIDATION_FILE, include_mag, magnetic_flux_images = 'distorted'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Plots the machine prediction time series of the for all the active regions\n",
    "    \n",
    "    Parameters =====>\n",
    "    1) VALIDATION_FILE : The cross validation file\n",
    "    2) include_mag : whehther to include the magnetic flux time series\n",
    "    3) magnetic_flux_images : Default = 'distorted' : The magnetic flux images for the distorted images or the original images\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    VALIDATION = convert_to_dictionary(VALIDATION_FILE)\n",
    "    \n",
    "    assert(magnetic_flux_images in ['original', 'distorted'])\n",
    "    \n",
    "    allSortedPaths = sort_active_region_files_wrt_time(VALIDATION)\n",
    "    #print(allSortedPaths.keys())\n",
    "    #print(allSortedPaths)\n",
    "    \n",
    "    pos_unsigned_flux = np.array([])\n",
    "    neg_unsigned_flux = np.array([])\n",
    "\n",
    "    uniqueActive = return_unique_active_regions(VALIDATION)\n",
    "    #print(len(uniqueActive))\n",
    "    if (magnetic_flux_images == 'original'):\n",
    "        ORIGINAL_CROSS_VALIDATION_FILE = distorted_cross_validation_to_original_cross_validation(VALIDATION)\n",
    "        ALL_ORIGINAL_SORTED_PATHS = sort_active_region_files_wrt_time(ORIGINAL_CROSS_VALIDATION_FILE)\n",
    "    #print(ALL_ORIGINAL_SORTED_PATHS)\n",
    "        \n",
    "    #print(ALL_ORIGINAL_SORTED_PATHS)\n",
    "        \n",
    "    \n",
    "    plt.figure(figsize=(15, math.ceil(len(uniqueActive[0:180])/3.0)*5 + math.ceil(len(uniqueActive)/3.0)))\n",
    "    \n",
    "    \n",
    "    if (magnetic_flux_images == 'original'):\n",
    "        IMAGE_DATA = ALL_ORIGINAL_SORTED_PATHS\n",
    "        \n",
    "    (global_f_max, global_f_min) = get_global_min_max_flux_values(IMAGE_DATA, uniqueActive, True)\n",
    "    \n",
    "    for i in range(180):\n",
    "        #print(i)\n",
    "        \n",
    "        if (include_mag == True):\n",
    "            f_data = get_mag_flux_array_for_one_active_region(IMAGE_DATA, i, True)\n",
    "            normalized = (f_data-global_f_min)/(global_f_max-global_f_min)\n",
    "        \n",
    "        (x_test, y_test) = get_sorted_data_for_particular_active_region(allSortedPaths, i)\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        \n",
    "        FLAG = get_flag(allSortedPaths.get(str(i))[0])\n",
    "        \n",
    "        if (FLAG == '1'):\n",
    "            REGION_TYPE = 'flaring'\n",
    "        else:\n",
    "            REGION_TYPE = 'nonflaring'\n",
    "        \n",
    "        plt.subplot(math.ceil(len(uniqueActive[0:180])/3.0),3,i+1)\n",
    "        plt.plot(y_pred)\n",
    "        if (include_mag == True):\n",
    "            plt.plot(normalized)\n",
    "        plt.xlabel('Time index')\n",
    "        plt.ylabel('Machine prediction')\n",
    "        plt.title(REGION_TYPE)\n",
    "        plt.ylim(0, 1.1)\n",
    "    \n",
    "    plt.savefig('time_series_original.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def magnetic_flux_with_prediction(VALIDATION_FILE, not_equal = False):\n",
    "    \n",
    "    pos_arr = np.array([])\n",
    "    neg_arr = np.array([])\n",
    "    \n",
    "    pos_unsigned_flux_arr = {}\n",
    "    pos_y_pred_glob = np.array([])\n",
    "    pos_mag_flux_glob = np.array([])\n",
    "    neg_y_pred_glob = np.array([])\n",
    "    neg_mag_flux_glob = np.array([])\n",
    "    pos_std_dev = np.array([])\n",
    "    neg_std_dev = np.array([])\n",
    "    \n",
    "    pos_mag_flux = {}\n",
    "    neg_mag_flux = {}\n",
    "    \n",
    "    for i in range(10):\n",
    "        pos_mag_flux[str(i)] = np.array([])\n",
    "        neg_mag_flux[str(i)] = np.array([])\n",
    "    \n",
    "    VALIDATION = convert_to_dictionary(VALIDATION_FILE)\n",
    "    \n",
    "    uniqueActive = return_unique_active_regions(VALIDATION)\n",
    "    allSortedPaths = sort_active_region_files_wrt_time(VALIDATION)\n",
    "    \n",
    "    pos_flux = [0]*10\n",
    "    neg_flux = [0]*10\n",
    "    pos_std = [0]*10\n",
    "    neg_std = [0]*10\n",
    "    \n",
    "    \n",
    "    for i in range(len(uniqueActive)):\n",
    "        (x_test, y_test) = get_sorted_data_for_particular_active_region(allSortedPaths, i)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        \n",
    "        FLAG = get_flag(allSortedPaths.get(str(i))[0])\n",
    "        temp_flux = np.array([])\n",
    "        \n",
    "        ALL_ORIGINAL_SORTED_PATHS = distorted_paths_dictionary_to_original_paths_dictionary(allSortedPaths, uniqueActive)\n",
    "        \n",
    "        FILES = ALL_ORIGINAL_SORTED_PATHS.get(str(i))\n",
    "        if (not_equal == True):\n",
    "            for m in range(len(FILES)):\n",
    "                x_data = np.load(FILES[m])\n",
    "                if (FLAG == '1'):\n",
    "                    pos_arr = np.append(pos_arr, y_pred[m])\n",
    "                    pos_mag_flux_glob = np.append(pos_mag_flux_glob, np.sum(np.abs(x_data)))\n",
    "                else:\n",
    "                    neg_arr = np.append(neg_arr, y_pred[m])\n",
    "                    neg_mag_flux_glob = np.append(neg_mag_flux_glob, np.sum(np.abs(x_data)))\n",
    "        else:\n",
    "            if (FLAG == '1'):\n",
    "                pos_arr = np.append(pos_arr, y_pred)\n",
    "                for i in range(len(y_pred)):\n",
    "                    pos_mag_flux_glob = np.append(pos_mag_flux_glob, np.sum(np.abs(x_test[i])))\n",
    "            else:\n",
    "                neg_arr = np.append(neg_arr, y_pred)\n",
    "                for i in range(len(y_pred)):\n",
    "                    neg_mag_flux_glob = np.append(neg_mag_flux_glob, np.sum(np.abs(x_test[i])))\n",
    "        \n",
    "\n",
    "        for m in range(len(pos_arr)):\n",
    "            for n in range(10):\n",
    "                if (np.logical_and( pos_arr[m] > n*0.1, pos_arr[m] <= (n*0.1+0.1) )):\n",
    "                    pos_mag_flux[str(n)] = np.append(pos_mag_flux[str(n)], pos_mag_flux_glob[m])\n",
    "\n",
    "        for m in range(len(neg_arr)):\n",
    "            for n in range(10):\n",
    "                if (np.logical_and( neg_arr[m] > n*0.1, neg_arr[m] <= (n*0.1+0.1) )):\n",
    "                    neg_mag_flux[str(n)] = np.append(neg_mag_flux[str(n)], neg_mag_flux_glob[m])\n",
    "                    \n",
    "                \n",
    "        for i in range(10):\n",
    "            pos_flux[i] = np.mean(pos_mag_flux[str(i)])\n",
    "            pos_std[i] = 10*np.std(pos_mag_flux[str(i)])/np.sqrt(len(pos_mag_flux[str(i)]))\n",
    "            neg_flux[i] = np.mean(neg_mag_flux[str(i)])\n",
    "            neg_std[i] = 10*np.std(neg_mag_flux[str(i)])/(np.sqrt(len(neg_mag_flux[str(i)])))\n",
    "        \n",
    "    return (pos_flux, neg_flux, pos_std, neg_std)\n",
    "\n",
    "\n",
    "def select_files_of_specified_machine_prediction(crossValidationFile):\n",
    "    data = {}\n",
    "    for i in range(10):\n",
    "        data[str(i)] = np.array([])\n",
    "    \n",
    "    validation = convert_to_dictionary(crossValidationFile)\n",
    "    \n",
    "    testData = validation.get(str(0))\n",
    "    \n",
    "    for i in range(len(testData)):\n",
    "        xData = np.load(testData[i])\n",
    "        xData = xData.reshape(1, 256, 256, 1)\n",
    "        yData = model.predict(xData)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if (np.logical_and(yData > m*0.0, yData <= (m*0.1 + 0.1))):\n",
    "                data[str(m)] = np.append(data[str(m)], testData[i])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5813399763423323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shamik/anaconda2/lib/python2.7/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "(validation, mean, std) = load_files()\n",
    "model = load_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../shared/Data/HMI_LOS_SHARPS/valid_magnetograms/los/415_20110313T1424_1.dat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.item().get(str(0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = convert_to_dictionary(validation)\n",
    "sortedActiveRegions = sort_active_region_files_wrt_time(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_rounder(t):\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour)\n",
    "               +datetime.timedelta(hours=t.minute//30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_active_region_times():\n",
    "    activeRegions = return_unique_active_regions(validation)\n",
    "    allSortedPaths = sort_active_region_files_wrt_time(validation)\n",
    "    dates = {}\n",
    "    for activeRegion in activeRegions:\n",
    "        dates[activeRegion] = []\n",
    "        for k in range(len(allSortedPaths[activeRegion])):\n",
    "            path = allSortedPaths[activeRegion][k]\n",
    "            FILE_NAME = os.path.split(path)[-1]\n",
    "            start = FILE_NAME.index('_')\n",
    "            end = FILE_NAME.index('_', start+1)\n",
    "            TIMESTAMP = FILE_NAME[start+1:end]\n",
    "            TIME = datetime.datetime.strptime(TIMESTAMP,'%Y%m%dT%H%M')\n",
    "            #dTS = time.mktime(TIME.timetuple())\n",
    "            dates[activeRegion].append(TIME)\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted_active_region_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aligned samples\n",
    "\n",
    "def flareReader(dataPath, fname):\n",
    "    with open(dataPath + '%s.txt' %fname) as fl:\n",
    "        txtData = fl.readlines()\n",
    "    txtData = [x.strip() for x in txtData]\n",
    "    data = {}\n",
    "    for entry in txtData:\n",
    "        splits = entry.split('\\t')\n",
    "        arrNum = int(splits[0])\n",
    "        data[arrNum] = []\n",
    "        for item in splits[1:]:\n",
    "            date = datetime.datetime.strptime(item.split(',')[0], '%Y-%m-%dT%H:%M:%S')\n",
    "            data[arrNum].append(date)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlignedSamples(allDates):\n",
    "    dataPath = \"\"\n",
    "    fname = \"flareData\"\n",
    "\n",
    "    flareData = flareReader(dataPath, fname) # get the time data for all the active regions\n",
    "    #allDates = sorted_active_region_times() # get the sorted active region datetimes for the individual active regions \n",
    "    \n",
    "    \n",
    "    \n",
    "    allRoundedDates = {}\n",
    "    \n",
    "    for k in allDates.keys():\n",
    "        tempDates = allDates[k]\n",
    "        allRoundedDates[k] = []\n",
    "        for i in range(len(tempDates)):\n",
    "            allRoundedDates[k].append(hour_rounder(tempDates[i]))\n",
    "    \n",
    "    \n",
    "    tempActiveRegions = return_unique_active_regions(validation) # returns the active regions for the cross validation file\n",
    "    \n",
    "    activeRegions = [activeRegion for activeRegion in tempActiveRegions if int(activeRegion) in flareData.keys()] # only keeps the \n",
    "    # active regions which are present at the flaringData file also. So, this includes only the flaring active regions\n",
    "    tw = 72 # how many files to include. Time span of the data\n",
    "    allData = {} # contains the 72 hours paths for all the active regions\n",
    "    \n",
    "    \n",
    "    print(len(activeRegions))\n",
    "    for activeRegion in activeRegions:\n",
    "        allData[activeRegion] = {} [0]*(2*tw + 1) # contains the 72 hours paths for each active region\n",
    "        dates = allRoundedDates[activeRegion] # timestamps for the current active region\n",
    "        maxLen = len(dates)\n",
    "        firstTS = dates[0] # start time of the current active region\n",
    "        lastTS = dates[-1] # end time of the current active region\n",
    "        \n",
    "        \n",
    "        print(len(flareData[int(activeRegion)]))\n",
    "        flareDate = flareData[int(activeRegion)][0] # first flare time of the current active region\n",
    "        \n",
    "        \n",
    "        for k in range(len(flareData[int(activeRegion)])):\n",
    "            allData[activeRegion][k] = [0]*(2*tw + 1)\n",
    "            \n",
    "            \n",
    "        maxIndex = tw*2\n",
    "        xMid = maxIndex / 2\n",
    "\n",
    "        \n",
    "        if hour_rounder(flareDate) in dates:\n",
    "            allData[activeRegion][xMid] = flareDate\n",
    "            flareIndex = dates.index(hour_rounder(flareDate))\n",
    "            \n",
    "            if (xMid > flareIndex):\n",
    "                dateStartIndex = 0\n",
    "                xStartIndex = xMid - flareIndex\n",
    "            else:\n",
    "                xStartIndex = 0\n",
    "                dateStartIndex = flareIndex - xMid\n",
    "                \n",
    "            if (xMid > (maxLen - flareIndex)):\n",
    "                dateEndIndex = maxLen - 1\n",
    "                xEndIndex = xMid + (maxLen - flareIndex)\n",
    "            else:\n",
    "                xEndIndex = maxIndex\n",
    "                dateEndIndex = flareIndex + xMid\n",
    "                \n",
    "            \n",
    "            allData[activeRegion][xStartIndex:xMid] = allDates[activeRegion][dateStartIndex:flareIndex]\n",
    "            allData[activeRegion][xMid+1:xEndIndex+1] = allDates[activeRegion][flareIndex+1:dateEndIndex+1]\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "38\n",
      "14\n",
      "3\n",
      "5\n",
      "1\n",
      "7\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "5\n",
      "24\n",
      "1\n",
      "1\n",
      "7\n",
      "2\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "alignedTimeData = getAlignedSamples(allDates)\n",
    "#alignedTimeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6ad8ed7de66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mallDates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_active_region_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mallDates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "allDates = sorted_active_region_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2015, 4, 17, 21, 0), datetime.datetime(2015, 4, 17, 22, 0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allDates['5472'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
